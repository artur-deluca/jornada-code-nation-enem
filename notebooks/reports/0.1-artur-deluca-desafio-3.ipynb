{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rebuild ENEM's answers<br></h3>\n",
    "\n",
    "The enem's test with 45 single choice math questions, followed by alternatives ranging from A to E. In this scenario the last five answers have been removed from the test dataset, so you will rebuild them from the final average result - creating a model to predict the marked down answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.models.score import score, naive_approach\n",
    "from src.models.mode_prediction import predict_answers\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "train = pd.read_csv('../../data/interim/train3.csv').set_index('NU_INSCRICAO')\n",
    "test  = pd.read_csv('../../data/interim/test3.csv').set_index('NU_INSCRICAO')\n",
    "validation  = pd.read_csv('../../data/interim/validation3.csv').set_index('NU_INSCRICAO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Strategy</h3><br>\n",
    "Considering the available options to choose from (A to E) and including the possibility to leave the question blank (*) the student last 5 answers would have a 1/7776 probability in a uniform distribution.<br><br> The strategy to overcome the dataset diversity consists on segmenting data in subsets in which the likelihood of similar answers gets increased. Not only segmenting the dataset by the types of tests employed, but by using the model defined in the previous challenge to recreate the math grades, we may better segment the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Estimate</h3><br>\n",
    "The underlying idea of the following function is to predict the most common written answer for the segmented performance quartile as well as for its corresponding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive approach accuracy: 20.07%\n",
      "Traning set accuracy: 32.27%\n",
      "Validation set accuracy: 32.16%\n"
     ]
    }
   ],
   "source": [
    "predict_n = 5\n",
    "train_answers = predict_answers(train, train)\n",
    "validation_answers = predict_answers(train, validation)\n",
    "print('Naive approach accuracy: {:.2f}%'.format(naive_approach(train)*100))\n",
    "print('Traning set accuracy: {:.2f}%'.format(score(train.TX_RESPOSTAS_MT.str[-predict_n:], train_answers)*100))\n",
    "print('Validation set accuracy: {:.2f}%'.format(score(validation.TX_RESPOSTAS_MT.str[-predict_n:], validation_answers)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open stored markov models\n",
    "try:\n",
    "    model = load_model()\n",
    "# trains and saves the models if the models are not stored\n",
    "except FileNotFoundError:\n",
    "    __train_whole_chain(train)\n",
    "    # open stored markov models\n",
    "    model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = lambda df, code, quartile: df.loc[(df['CO_PROVA_MT'] == code) & (df['MT_QT'] == quartile)].index\n",
    "test_codes = train.CO_PROVA_MT.unique()\n",
    "groups = train.group.unique()\n",
    "\n",
    "# iterate through all the math test codes\n",
    "for code in test_codes:\n",
    "    # iterate through the classified groups\n",
    "    for group in groups:\n",
    "        key = tuple([code, group])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = ['CO_PROVA_MT', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('30259880377aeacdd636a7adda0f65287f35d140', 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

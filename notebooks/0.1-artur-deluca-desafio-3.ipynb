{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rebuild ENEM's answers<br></h3>\n",
    "\n",
    "Since some ENEM answers have been lost, you will rebuild them from the final average result - creating a model to predict the marked down answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# could not manage to use a package version of refactor modules\n",
    "sys.path.insert(0, '../src')\n",
    "from send_answer import send_answer\n",
    "\n",
    "sys.path.insert(0, '../src/models')\n",
    "from regression import predict\n",
    "from score import score\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input data\n",
    "train = pd.read_csv('../data/raw/train.csv', index_col=0).set_index('NU_INSCRICAO')\n",
    "test = pd.read_csv('../data/raw/test3.csv').set_index('NU_INSCRICAO')\n",
    "\n",
    "# quick data clean-up\n",
    "train.loc[:,'TX_RESPOSTAS_MT'] = train.loc[:,'TX_RESPOSTAS_MT'].str.replace('\\.','*')\n",
    "train = train.loc[train.TX_RESPOSTAS_MT.dropna(axis=0).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous challenge, the strategy consists on recreating the math grades in order to better segment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict the grades on the test set using the Quantile Transformation\n",
    "grade_prediction = predict(train.drop('TX_RESPOSTAS_MT', axis=1), test.drop('TX_RESPOSTAS_MT', axis=1))\n",
    "test.loc[list(grade_prediction.index), 'NU_NOTA_MT'] = grade_prediction.loc[:,'NU_NOTA_MT']\n",
    "\n",
    "# remove the 0 scores from the training set\n",
    "train = train.loc[train.NU_NOTA_MT != 0,:]\n",
    "\n",
    "# reposition the training set\n",
    "train = train.copy()[list(test.columns)+['TX_GABARITO_MT']]\n",
    "\n",
    "# separte the datasets in quartiles based on the math grade\n",
    "quartiles = 4\n",
    "merged_grades = pd.qcut(pd.concat([train.NU_NOTA_MT, test.NU_NOTA_MT]), quartiles, labels = False)\n",
    "train['MT_QT'] = merged_grades.loc[train.index].values\n",
    "test['MT_QT'] = merged_grades.loc[test.index].values\n",
    "\n",
    "predict_n = 5\n",
    "\n",
    "train['PREDICTION'] = ''\n",
    "test['PREDICTION'] = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prediction strategy</h3><br>\n",
    "The underlying idea of the following function is to predict the most common written answer for the segmented performance quartile as well as for its corresponding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_answers(dataset, predict_n=5):\n",
    "    \n",
    "    df = dataset.copy()\n",
    "    # iterate through each type of math test in the training set\n",
    "    for code in df.CO_PROVA_MT.unique():\n",
    "        \n",
    "        # iterate through each quartile in this math test\n",
    "        for quartile in df.loc[df.CO_PROVA_MT == code, 'MT_QT'].unique():\n",
    "            \n",
    "            enem_answer = ''\n",
    "            filter = (df.CO_PROVA_MT == code) & (df.MT_QT == quartile)\n",
    "            for _ in range(predict_n):\n",
    "            \n",
    "                # accumulate the last n-enem_answers for each row\n",
    "                enem_answer += df.loc[filter, 'TX_RESPOSTAS_MT'].str[-predict_n+len(enem_answer)].mode()[0]\n",
    "            df.loc[filter, 'PREDICTION'] = enem_answer\n",
    "    return df\n",
    "\n",
    "def naive_approach(dataset, predict_n = 5):\n",
    "    df = dataset.copy()\n",
    "    for i in df.index:\n",
    "        df.loc[i,'PREDICTION'] = df.loc[i,'PREDICTION'].join(''.join(str(x) for x in np.random.choice(['A','B','C','D','E'], size=predict_n)))\n",
    "    print('Naive approach: %.2f' % (score(df.TX_RESPOSTAS_MT.str[-predict_n:], df.PREDICTION)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive approach: 20.08\n",
      "Traning set accuracy: 32.30\n"
     ]
    }
   ],
   "source": [
    "# define the number of questions to estimate\n",
    "\n",
    "naive_approach(train)\n",
    "train = predict_answers(train)\n",
    "test = predict_answers(test)\n",
    "print('Traning set accuracy: %.2f' % (score(train.TX_RESPOSTAS_MT.str[-predict_n:], train.PREDICTION)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = test.copy().loc[:,['PREDICTION']]\n",
    "answer = answer.rename(index=str, columns={\"PREDICTION\": \"TX_RESPOSTAS_MT\"})\n",
    "#send_answer(answer.reset_index(), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

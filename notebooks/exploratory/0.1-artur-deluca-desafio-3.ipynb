{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rebuild ENEM's answers<br></h3>\n",
    "\n",
    "The enem's test with 45 single choice math questions, followed by alternatives ranging from A to E. In this scenario the last five answers have been removed from the test dataset, so you will rebuild them from the final average result - creating a model to predict the marked down answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.send_answer import send_answer\n",
    "from src.features.build_features import estimate_math\n",
    "from src.models.score import score, naive_approach\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "train = pd.read_csv('../../data/raw/train.csv', index_col=0).set_index('NU_INSCRICAO')\n",
    "test = pd.read_csv('../../data/raw/test3.csv').set_index('NU_INSCRICAO')\n",
    "\n",
    "# quick data clean-up\n",
    "train.loc[:,'TX_RESPOSTAS_MT'] = train.loc[:,'TX_RESPOSTAS_MT'].str.replace('\\.','*')\n",
    "train = train.loc[train.TX_RESPOSTAS_MT.dropna(axis=0).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Strategy</h3><br>\n",
    "Considering the available options to choose from (A to E) and including the possibility to leave the question blank (*) the student last 5 answers would have a 1/7776 probability in a uniform distribution.<br><br> The strategy to overcome the dataset diversity consists on segmenting data in subsets in which the likelihood of similar answers gets increased. Not only segmenting the dataset by the types of tests employed, but by using the model defined in the previous challenge to recreate the math grades, we may better segment the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the grades on the test set using the Quantile Transformation\n",
    "grade_prediction = estimate_math(train.drop('TX_RESPOSTAS_MT', axis=1), test.drop('TX_RESPOSTAS_MT', axis=1))\n",
    "test.loc[list(grade_prediction.index), 'NU_NOTA_MT'] = grade_prediction.loc[:,'NU_NOTA_MT']\n",
    "\n",
    "# remove the 0 scores from the training set\n",
    "train = train.loc[train.NU_NOTA_MT != 0,:]\n",
    "\n",
    "# reposition the training set\n",
    "train = train.copy()[list(test.columns)+['TX_GABARITO_MT']]\n",
    "\n",
    "# separte the datasets in quartiles based on the math grade\n",
    "quartiles = 4\n",
    "merged_grades = pd.qcut(pd.concat([train.NU_NOTA_MT, test.NU_NOTA_MT]), quartiles, labels = False)\n",
    "train['MT_QT'] = merged_grades.loc[train.index].values\n",
    "test['MT_QT'] = merged_grades.loc[test.index].values\n",
    "\n",
    "predict_n = 5\n",
    "\n",
    "train['PREDICTION'] = ''\n",
    "test['PREDICTION'] = '' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Estimate</h3><br>\n",
    "The underlying idea of the following function is to predict the most common written answer for the segmented performance quartile as well as for its corresponding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answers(dataset, predict_n=5):\n",
    "    \n",
    "    df = dataset.copy()\n",
    "    # iterate through each type of math test in the training set\n",
    "    for code in df.CO_PROVA_MT.unique():\n",
    "        \n",
    "        # iterate through each quartile in this math test\n",
    "        for quartile in df.loc[df.CO_PROVA_MT == code, 'MT_QT'].unique():\n",
    "            \n",
    "            enem_answer = ''\n",
    "            filter = (df.CO_PROVA_MT == code) & (df.MT_QT == quartile)\n",
    "            for _ in range(predict_n):\n",
    "            \n",
    "                # accumulate the last n-enem_answers for each row\n",
    "                enem_answer += df.loc[filter, 'TX_RESPOSTAS_MT'].str[-predict_n+len(enem_answer)].mode()[0]\n",
    "            df.loc[filter, 'PREDICTION'] = enem_answer\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive approach accuracy: 19.90%\n",
      "Traning set accuracy: 32.32%\n"
     ]
    }
   ],
   "source": [
    "train_answers = predict_answers(train)\n",
    "test_answers = predict_answers(test)\n",
    "print('Naive approach accuracy: {:.2f}%'.format(naive_approach(train)*100))\n",
    "print('Traning set accuracy: {:.2f}%'.format(score(train_answers.TX_RESPOSTAS_MT.str[-predict_n:], train_answers.PREDICTION)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send answers\n",
    "answer = test_answers.copy().loc[:,['PREDICTION']]\n",
    "answer = answer.rename(index=str, columns={\"PREDICTION\": \"TX_RESPOSTAS_MT\"})\n",
    "#send_answer(answer.reset_index(), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
